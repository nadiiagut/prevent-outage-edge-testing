# Output Artifacts

POET generates several artifacts to help you build and validate tests. This guide explains each output and how to use it.

---

## Overview

| Artifact | Location | Purpose |
|----------|----------|---------|
| [TESTPLAN.md](#testplanmd) | `generated/TESTPLAN.md` | Human-readable test plan |
| [tests/](#generated-tests) | `generated/tests/` | Pytest starter scaffolds |
| [observability/](#observability-runbooks) | `generated/observability/` | Monitoring and debugging recipes |
| [snippets/](#code-snippets) | `generated/snippets/` | Reusable code helpers |
| [Gate Reports](#gate-reports) | `.poet/reports/` | Pass/fail validation results |

---

## TESTPLAN.md

A structured test plan document linking features to failure modes and test cases.

### Structure

```markdown
# Feature Title

> Generated by POET on 2026-02-05 09:00:00

## Source Description
[Original feature description]

## Failure Modes Covered
- `cache-key-collision`
- `vary-header-cache-split`
- `stale-on-revalidate-fail`

## Test Cases

### 1. Test Case Name
**Priority:** critical | high | medium | low
**Failure Mode:** `failure-mode-id`
**Obligation:** `obligation.id`

[Description of what this test validates]

**Setup:**
1. Configure origin with specific response headers
2. Clear edge cache
3. Start traffic capture

**Execution:**
1. Send request A with specific headers
2. Record response
3. Send request B with different headers
4. Record response

**Assertions:**
- [ ] Response A has X-Cache: MISS
- [ ] Response B has X-Cache: HIT
- [ ] Cache key includes Vary dimension

**Evidence Required:**
- Access log with cache status
- Response headers capture
- (Optional) Packet capture

**Cleanup:**
1. Purge test cache entries
2. Stop traffic capture

---

### 2. Next Test Case...
```

### How to Use

1. **Review** — Read through to understand test coverage
2. **Prioritize** — Focus on critical/high priority tests first
3. **Implement** — Use as spec for writing actual tests
4. **Track** — Check off assertions as tests pass
5. **Archive** — Commit with feature PR for documentation

### Customization

The test plan is generated, not sacred. You should:

- Add domain-specific setup steps
- Remove irrelevant test cases
- Adjust assertions for your environment
- Add business logic tests POET can't infer

---

## Generated Tests

Pytest starter files with TODO markers. **These are scaffolds, not complete tests.**

### Location

```
generated/tests/
├── conftest.py           # Shared fixtures
├── test_cache_bypass.py  # Test file per failure mode
├── test_vary_handling.py
└── ...
```

### Structure

```python
# generated/tests/test_cache_bypass.py
"""
Tests for cache bypass behavior.

Generated by POET from: EDGE-123
Failure modes covered:
  - cache-key-collision
  - conditional-request-bypass

TODO: Complete the implementation below.
"""

import pytest


@pytest.fixture
def edge_client():
    """
    TODO: Initialize HTTP client pointing to edge proxy.
    
    Example:
        return httpx.Client(base_url="http://edge.local:8080")
    """
    raise NotImplementedError("Complete this fixture")


class TestCacheBypass:
    """Tests for cache bypass with Authorization header."""
    
    def test_auth_header_bypasses_cache(self, edge_client):
        """
        Requests with Authorization header must not be served from cache.
        
        Failure mode: cache-key-collision
        Obligation: cache.key.stability
        
        TODO: Implement this test.
        """
        # Setup
        # TODO: Configure origin to return cacheable response
        
        # Execute
        # TODO: Send request with Authorization header
        # response = edge_client.get("/api/resource", headers={"Authorization": "Bearer token"})
        
        # Assert
        # TODO: Verify cache miss
        # assert response.headers.get("X-Cache") == "MISS"
        
        raise NotImplementedError("Complete this test")
    
    def test_no_auth_uses_cache(self, edge_client):
        """
        Requests without Authorization header should use cache.
        
        TODO: Implement this test.
        """
        raise NotImplementedError("Complete this test")
```

### TODO Markers

Every generated test includes:

| Marker | Meaning |
|--------|---------|
| `TODO: Complete this fixture` | Fixture needs implementation |
| `TODO: Implement this test` | Test body needs implementation |
| `TODO: Configure origin...` | Setup step needs your environment details |
| `raise NotImplementedError` | Ensures test fails until completed |

### How to Use

1. **Copy** to your test directory (or generate in place with `--output ./tests`)
2. **Complete fixtures** — Add your actual client/server setup
3. **Implement tests** — Replace TODOs with real assertions
4. **Remove NotImplementedError** — Once test is complete
5. **Run** — `pytest generated/tests/`

### Why Scaffolds, Not Complete Tests?

POET doesn't know:
- Your actual endpoint URLs
- Your authentication mechanism
- Your origin server setup
- Your specific header names
- Your deployment environment

Scaffolds give you structure; you add the specifics.

---

## Observability Runbooks

Recipes for collecting evidence and debugging failures.

### Location

```
generated/observability/
├── cache-metrics.md      # What Prometheus metrics to check
├── access-log-fields.md  # Required log fields
├── packet-capture.md     # tcpdump commands
└── dtrace-latency.md     # DTrace scripts (if available)
```

### Structure

```markdown
# Cache Metrics Runbook

## Purpose
Monitor cache hit ratio and detect cache bypass issues.

## Prometheus Queries

### Cache Hit Ratio
```promql
sum(rate(cache_hits_total[5m])) / sum(rate(cache_requests_total[5m]))
```

### Cache Bypass Rate
```promql
sum(rate(cache_bypass_total{reason="authorization"}[5m]))
```

## Alert Thresholds

| Metric | Warning | Critical |
|--------|---------|----------|
| Cache hit ratio | < 80% | < 50% |
| Bypass rate | > 10% | > 25% |

## Debugging Steps

1. Check if Authorization header is present in requests
2. Verify Cache-Control headers from origin
3. Review cache key configuration
```

### How to Use

1. **Before testing** — Set up monitoring per runbook
2. **During testing** — Collect evidence artifacts
3. **After testing** — Attach evidence to gate report
4. **On failure** — Use debugging steps to investigate

---

## Code Snippets

Reusable helper code for common tasks.

### Location

```
generated/snippets/
├── cache_validator.py    # Verify cache headers
├── latency_analyzer.py   # Analyze timing data
├── fault_injector.py     # Inject failures
└── log_parser.py         # Parse access logs
```

### Example: Cache Validator

```python
# generated/snippets/cache_validator.py
"""
Helper for validating cache behavior.

Usage:
    from cache_validator import CacheValidator
    
    validator = CacheValidator(base_url="http://edge.local:8080")
    result = validator.check_cache_status("/api/resource")
    assert result.is_hit
"""

import httpx
from dataclasses import dataclass
from typing import Optional


@dataclass
class CacheResult:
    status: str  # HIT, MISS, STALE, BYPASS
    is_hit: bool
    is_miss: bool
    age: Optional[int]
    cache_control: Optional[str]


class CacheValidator:
    def __init__(self, base_url: str):
        self.client = httpx.Client(base_url=base_url)
    
    def check_cache_status(self, path: str, **kwargs) -> CacheResult:
        response = self.client.get(path, **kwargs)
        x_cache = response.headers.get("X-Cache", "UNKNOWN")
        return CacheResult(
            status=x_cache,
            is_hit=x_cache == "HIT",
            is_miss=x_cache == "MISS",
            age=int(response.headers.get("Age", 0)) if "Age" in response.headers else None,
            cache_control=response.headers.get("Cache-Control"),
        )
```

### How to Use

1. **Copy** snippets to your project
2. **Import** in your tests
3. **Customize** for your environment
4. **Extend** as needed

---

## Gate Reports

Pass/fail validation results with evidence references.

### Locations

| File | Format | Purpose |
|------|--------|---------|
| `.poet/reports/latest.json` | JSON | Machine-readable results |
| `.poet/reports/latest.html` | HTML | Human-readable report |
| `.poet/reports/{timestamp}.json` | JSON | Historical reports |

### JSON Structure

```json
{
  "timestamp": "2026-02-05T09:00:00Z",
  "overall_status": "passed",
  "summary": {
    "total_gates": 4,
    "passed": 3,
    "failed": 1,
    "skipped": 0,
    "total_duration_ms": 1234
  },
  "gates": [
    {
      "gate_id": "contract",
      "gate_name": "Contract Tests",
      "status": "passed",
      "duration_ms": 456,
      "checks": [
        {
          "name": "cache.key.stability",
          "status": "passed",
          "message": "All 3 assertions passed",
          "evidence": [
            ".poet/evidence/cache_key_test_access.log",
            ".poet/evidence/cache_key_test_response.json"
          ]
        }
      ]
    },
    {
      "gate_id": "performance",
      "gate_name": "Performance Baseline",
      "status": "failed",
      "duration_ms": 789,
      "checks": [
        {
          "name": "p99_latency",
          "status": "failed",
          "message": "P99 latency 150ms exceeds threshold 100ms",
          "evidence": [
            ".poet/evidence/latency_histogram.csv"
          ]
        }
      ]
    }
  ]
}
```

### Human-Readable Output

```
Gate Results
============

✓ Contract Tests (contract)
  ✓ cache.key.stability: All 3 assertions passed
  ✓ cache.vary.honored: All 2 assertions passed

✗ Performance Baseline (performance)
  ✗ p99_latency: P99 latency 150ms exceeds threshold 100ms

− Observability (observability)
  − Skipped: No evidence directory found

Summary: 2 passed, 1 failed, 1 skipped
Duration: 1.2s

Reports saved:
  JSON: .poet/reports/latest.json
  HTML: .poet/reports/latest.html
```

### How to Use

```bash
# Run all gates
poet gate run --all

# Run specific gate
poet gate run --gate contract

# View latest report
poet gate report

# Get JSON output
poet gate report --json

# Use in CI (exit code reflects pass/fail)
poet gate run --all || exit 1
```

### Evidence Directory

Gate runs collect evidence in `.poet/evidence/`:

```
.poet/evidence/
├── cache_key_test_access.log    # Access logs
├── cache_key_test_response.json # Response captures
├── latency_histogram.csv        # Performance data
└── packet_capture.pcap          # Network traces
```

---

## Output Directory Structure

Default output layout:

```
generated/                    # Default output directory
├── TESTPLAN.md              # Test plan document
├── tests/                   # Pytest scaffolds
│   ├── conftest.py
│   └── test_*.py
├── observability/           # Monitoring recipes
│   └── *.md
└── snippets/                # Code helpers
    └── *.py

.poet/                       # Runtime directory (git-ignored)
├── profile.json             # System capabilities
├── evidence/                # Collected evidence
│   └── *.log, *.json, *.csv
└── reports/                 # Gate reports
    ├── latest.json
    ├── latest.html
    └── {timestamp}.json
```

### Customizing Output Location

```bash
# Custom output directory
poet build --jira-file spec.md --output ./my-feature-tests

# Outputs to:
# ./my-feature-tests/TESTPLAN.md
# ./my-feature-tests/tests/
# ./my-feature-tests/observability/
# ./my-feature-tests/snippets/
```

---

## See Also

- [docs/inputs.md](inputs.md) — Input mode documentation
- [docs/jira.md](jira.md) — Jira integration guide
- [README.md](../README.md) — Project overview
- [examples/](../examples/) — Sample outputs
